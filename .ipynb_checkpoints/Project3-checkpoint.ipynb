{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3 - Webscraping Linkedin.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background\n",
    "\n",
    "Linkedin.com has very well presented job postings for web scraping. This is a demonstration of how to scrape from the job postings page on LinkedIn.com using Python. We'll pull the first 1,000 job postings since the site limits the amount of jobs it will load to that number.  Let's load the libraries we'll use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import bs4\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the Webdriver from the Selenium library\n",
    "\n",
    "First we need to find a way to automate the web browsing to scrape each page on the website. Otherwise, it could take quite some time. Let's store our driver as an object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "browser = webdriver.Chrome(executable_path=r\"/Users/chesterpoon/chromedriver\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Navigating to the Jobs Page\n",
    "\n",
    "With the next few lines of code, we will now automatically login to linkedin.com and navigate to the jobs page, where we'll instruct Selenium to search for \"data scientist\". To do this, we'll take the following steps:\n",
    "\n",
    "-  Find the username element and enter a username\n",
    "-  Find the password element and enter the password\n",
    "-  Find the \"login\" button and click it\n",
    "-  Find the jobs icon on the homepage and click it\n",
    "-  Find the search box and enter \"data scientist\" in the field\n",
    "-  Find the \"Search\" button and click it to initiate the search\n",
    "\n",
    "The below code starts this process. You may notice a while loop added to the code. For a reason that I cannot explain, an error would be thrown saying it could not find the element for the search box. However, when I ran the code a second time without any changes, it worked. The while loop was created to keep trying until an error was no longer thrown. Linkedin's job posting page has a couple of these, thus any instance in the code where a while loop is in place is meant to handle the errors thrown. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "browser.get('https://www.linkedin.com/')\n",
    "log_name = browser.find_element_by_id('login-email')\n",
    "log_name.send_keys('username@email.com')\n",
    "log_pass = browser.find_element_by_id('login-password')\n",
    "log_pass.send_keys('password')\n",
    "login_but = browser.find_element_by_id('login-submit')\n",
    "login_but.click()\n",
    "jobs_link = browser.find_element_by_id('jobs-tab-icon')\n",
    "jobs_link.click()\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        search_jobs = browser.find_element_by_css_selector('input[id*=jobs-search-box-keyword-id-ember]')\n",
    "    except:\n",
    "        continue\n",
    "    break\n",
    "\n",
    "search_jobs = browser.find_element_by_css_selector('input[id*=jobs-search-box-keyword-id-ember]')\n",
    "search_jobs.send_keys('data scientist')\n",
    "search_go = browser.find_element_by_css_selector(\n",
    "    'button.jobs-search-box__submit-button.button-secondary-large-inverse')\n",
    "search_go.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scraping the Data from Each Posting\n",
    "\n",
    "Now comes the bulk of the work where we will need to iterate through each posting to gather the data. Here's how we will tackle this problem:\n",
    "\n",
    "-  We'll create some empty vectors (lists in 'Python talk') to store the values we gather in order to setup our dataframe later.\n",
    "-  Take all of the below steps and set up in a loop that repeats itself 40 times to not go over the 1,000 posting limit.\n",
    "> -  Because the job postings site is javascript enabled, we need to render all job postings for the first page. So we automate the action of scrolling to the bottom of the screen.\n",
    "> -  We then use the Selenium and Beautiful Soup libraries to gather the page source to parse the html.\n",
    "> -  Once the html is parsed, we can build a loop to extract the list of skills required for each posting while assigning a job id, the primary industry of the job, and the title of the job for the skillset.\n",
    "-  We'll then find the element of the button for the next page and repeat.\n",
    "-  Throughout these steps, we'll build in \"sleep\" functions to give the browser time to render the html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "skill_list = []\n",
    "title_list = []\n",
    "industry_list = []\n",
    "job_id = []\n",
    "\n",
    "#This for-loop is intended to iterate the navigation of pages\n",
    "for p in range(2,42):\n",
    "    def pg_dwn():\n",
    "        #This function is a page down function\n",
    "        while True:\n",
    "            try:\n",
    "                browser.find_element_by_css_selector(\n",
    "                    'div[class*=jobs-search-results--is-two-pane]').send_keys(Keys.PAGE_DOWN)\n",
    "            except:\n",
    "                continue\n",
    "            break\n",
    "\n",
    "    for i in range(25):\n",
    "        pg_dwn()\n",
    "\n",
    "    url = browser.current_url\n",
    "    source = browser.page_source\n",
    "    html = bs4.BeautifulSoup(source, \"lxml\")\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            #Find all the jobs\n",
    "            job = browser.find_elements_by_class_name('job-card-search__title-line')\n",
    "            jtitle1 = html.find_all(\n",
    "                attrs={\"class\": \"truncate-multiline--last-line-wrapper\"})\n",
    "            jtitle2 = html.find_all(\n",
    "                attrs={\"class\": \"job-card-search__title lt-line-clamp lt-line-clamp--multi-line ember-view\"})\n",
    "        except:\n",
    "            continue\n",
    "        break\n",
    "    \n",
    "    #Next few lines will combine promoted job ads and regular ones\n",
    "    title1 = []\n",
    "    title2 = []\n",
    "\n",
    "    for a in range(len(jtitle1)):\n",
    "        title1.append(jtitle1[a].getText())\n",
    "    for b in range(len(jtitle2)):\n",
    "        title2.append(jtitle2[b].getText())\n",
    "    \n",
    "    jtitle = title1 + title2\n",
    "    \n",
    "    j_id = 0\n",
    "    \n",
    "    #This code below clicks through each job ad and stores the information scraped.\n",
    "    for i in range(len(job)-1):\n",
    "        job[i].click()\n",
    "        time.sleep(2)\n",
    "        url = browser.current_url\n",
    "        source = browser.page_source\n",
    "        html = bs4.BeautifulSoup(source, \"lxml\")\n",
    "        skills = html.find_all(attrs={\"class\": \"jobs-ppc-criteria__value t-14 t-black t-normal ml2 block\"})\n",
    "        industry = html.find_all(attrs={\"class\": \"jobs-box__list-item jobs-description-details__list-item\"})\n",
    "        j_id = j_id + 1\n",
    "        \n",
    "        for j in range(len(skills)):\n",
    "            s = skills[j].getText()\n",
    "            t = jtitle[i]\n",
    "            ind = industry[0].getText()\n",
    "            skill_list.append(s)\n",
    "            title_list.append(t)\n",
    "            industry_list.append(ind)\n",
    "            job_id.append(\"LI\" + str(j_id))\n",
    "    \n",
    "    if p == 41:\n",
    "        print(\"Last page complete\")\n",
    "        break\n",
    "    \n",
    "    #Code below finds the button for the next page.\n",
    "    try:\n",
    "        page = browser.find_elements_by_xpath('//button/span[text()=\"' + str(p) +'\"]')\n",
    "    except:\n",
    "        page = browser.find_elements_by_xpath('//button/span[text()=\"â€¦\"]')\n",
    "    \n",
    "    try:\n",
    "        page[1].click()\n",
    "    except:\n",
    "        page[0].click()\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Our Data into a Data Frame\n",
    "\n",
    "Now that we have our data loaded up, we can store our data into a data frame and export it out as a csv file to prepare for the data cleaning process in R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "skillsdf = pd.DataFrame(\n",
    "    {'job_id': job_id,\n",
    "     'skills': skill_list, \n",
    "     'title': title_list, \n",
    "     \"industry\": industry_list})\n",
    "skillsdf.to_csv('skills.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "skillsdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
